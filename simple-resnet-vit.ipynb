{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10214857,"sourceType":"datasetVersion","datasetId":6313708}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"yakhyokhuja/webface-112x112\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:03:34.061917Z","iopub.execute_input":"2026-01-20T22:03:34.062520Z","iopub.status.idle":"2026-01-20T22:03:34.951421Z","shell.execute_reply.started":"2026-01-20T22:03:34.062487Z","shell.execute_reply":"2026-01-20T22:03:34.950449Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/webface-112x112\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.cuda.amp import autocast, GradScaler\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.ops import nms\n\nimport numpy as np\nimport cv2\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:33:53.464610Z","iopub.execute_input":"2026-01-20T22:33:53.465473Z","iopub.status.idle":"2026-01-20T22:34:01.624580Z","shell.execute_reply.started":"2026-01-20T22:33:53.465439Z","shell.execute_reply":"2026-01-20T22:34:01.623735Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=2048, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        \n    def forward(self, x):\n        x = self.proj(x)  # (B, embed_dim, H', W')\n        x = x.flatten(2)   # (B, embed_dim, n_patches)\n        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:34:53.109080Z","iopub.execute_input":"2026-01-20T22:34:53.109951Z","iopub.status.idle":"2026-01-20T22:34:53.116533Z","shell.execute_reply.started":"2026-01-20T22:34:53.109917Z","shell.execute_reply":"2026-01-20T22:34:53.115706Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        \n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:35:17.374869Z","iopub.execute_input":"2026-01-20T22:35:17.375179Z","iopub.status.idle":"2026-01-20T22:35:17.382652Z","shell.execute_reply.started":"2026-01-20T22:35:17.375154Z","shell.execute_reply":"2026-01-20T22:35:17.382066Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = MultiHeadAttention(dim, num_heads, qkv_bias, attn_drop, drop)\n        self.norm2 = nn.LayerNorm(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(drop),\n            nn.Linear(mlp_hidden_dim, dim),\n            nn.Dropout(drop)\n        )\n        \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:35:40.776114Z","iopub.execute_input":"2026-01-20T22:35:40.776973Z","iopub.status.idle":"2026-01-20T22:35:40.782994Z","shell.execute_reply.started":"2026-01-20T22:35:40.776941Z","shell.execute_reply":"2026-01-20T22:35:40.782294Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    \"\"\"Lightweight ViT for feature enhancement\"\"\"\n    def __init__(self, img_size=7, patch_size=1, in_chans=2048, embed_dim=768, \n                 depth=6, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, qkv_bias, drop_rate, drop_rate)\n            for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:38:51.490209Z","iopub.execute_input":"2026-01-20T22:38:51.491070Z","iopub.status.idle":"2026-01-20T22:38:51.497954Z","shell.execute_reply.started":"2026-01-20T22:38:51.491033Z","shell.execute_reply":"2026-01-20T22:38:51.497138Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        for blk in self.blocks:\n            x = blk(x)\n            \n        x = self.norm(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:39:28.709330Z","iopub.execute_input":"2026-01-20T22:39:28.710233Z","iopub.status.idle":"2026-01-20T22:39:28.714782Z","shell.execute_reply.started":"2026-01-20T22:39:28.710203Z","shell.execute_reply":"2026-01-20T22:39:28.714108Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class FPN(nn.Module):\n    \"\"\"Feature Pyramid Network for multi-scale detection\"\"\"\n    def __init__(self, in_channels_list, out_channels=256):\n        super().__init__()\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        \n        for in_channels in in_channels_list:\n            lateral_conv = nn.Conv2d(in_channels, out_channels, 1)\n            fpn_conv = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n            self.lateral_convs.append(lateral_conv)\n            self.fpn_convs.append(fpn_conv)\n            \n    def forward(self, inputs):\n        # Build top-down pathway\n        laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n        \n        # Top-down fusion\n        for i in range(len(laterals) - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(laterals[i], scale_factor=2, mode='nearest')\n            # Apply convolutions\n        outputs = [self.fpn_convs[i](laterals[i]) for i in range(len(laterals))]\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:40:15.393255Z","iopub.execute_input":"2026-01-20T22:40:15.393618Z","iopub.status.idle":"2026-01-20T22:40:15.400591Z","shell.execute_reply.started":"2026-01-20T22:40:15.393593Z","shell.execute_reply":"2026-01-20T22:40:15.399667Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class DetectionHead(nn.Module):\n    \"\"\"RetinaFace-style detection head for bounding boxes, landmarks, and confidence\"\"\"\n    def __init__(self, in_channels=256, num_anchors=3):\n        super().__init__()\n        self.num_anchors = num_anchors\n        \n        # Classification head\n        self.cls_head = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, num_anchors * 2, 3, padding=1)  # 2 classes (face/background)\n        )\n        \n        # Bounding box regression head\n        self.bbox_head = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, num_anchors * 4, 3, padding=1)  # 4 bbox coords\n        )\n        \n        # Landmark regression head (5 keypoints: eyes, nose, mouth corners)\n        self.landmark_head = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, num_anchors * 10, 3, padding=1)  # 5 points * 2 coords\n        )\n        \n    def forward(self, x):\n        cls_scores = self.cls_head(x)\n        bbox_preds = self.bbox_head(x)\n        landmark_preds = self.landmark_head(x)\n        \n        return cls_scores, bbox_preds, landmark_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:40:37.611190Z","iopub.execute_input":"2026-01-20T22:40:37.611595Z","iopub.status.idle":"2026-01-20T22:40:37.619506Z","shell.execute_reply.started":"2026-01-20T22:40:37.611569Z","shell.execute_reply":"2026-01-20T22:40:37.618789Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\nclass HybridFaceDetector(nn.Module):\n    \"\"\"Hybrid ResNet-50 + ViT Face Detection Model\"\"\"\n    def __init__(self, pretrained=True, num_anchors=3):\n        super().__init__()\n        \n        # ResNet-50 backbone (pre-trained on ImageNet)\n        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n        \n        # Extract multi-scale features\n        self.conv1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = resnet.layer1  # 256 channels, /4\n        self.layer2 = resnet.layer2  # 512 channels, /8\n        self.layer3 = resnet.layer3  # 1024 channels, /16\n        self.layer4 = resnet.layer4  # 2048 channels, /32\n        \n        # Vision Transformer for global attention (applied to layer4 output)\n        self.vit = VisionTransformer(\n            img_size=7,  # 224/32 = 7\n            patch_size=1,\n            in_chans=2048,\n            embed_dim=768,\n            depth=6,\n            num_heads=12\n        )\n        \n        # Projection to match ResNet channel dimension\n        self.vit_proj = nn.Conv2d(768, 2048, 1)\n        \n        # Feature fusion layer\n        self.fusion = nn.Sequential(\n            nn.Conv2d(4096, 2048, 1),  # Concatenated features\n            nn.BatchNorm2d(2048),\n            nn.ReLU(inplace=True)\n        )\n        \n        # FPN for multi-scale features\n        self.fpn = FPN([256, 512, 1024, 2048], out_channels=256)\n        \n        # Detection heads for each FPN level\n        self.detection_heads = nn.ModuleList([\n            DetectionHead(256, num_anchors) for _ in range(4)\n        ])\n        \n    def forward(self, x):\n        # ResNet feature extraction\n        c1 = self.conv1(x)\n        c2 = self.layer1(c1)  # /4\n        c3 = self.layer2(c2)  # /8\n        c4 = self.layer3(c3)  # /16\n        c5 = self.layer4(c4)  # /32\n        \n        # ViT enhancement on deepest features\n        B, C, H, W = c5.shape\n        vit_feat = self.vit(c5)  # (B, n_patches+1, 768)\n        vit_feat = vit_feat[:, 1:, :]  # Remove cls token\n        vit_feat = vit_feat.transpose(1, 2).reshape(B, 768, H, W)\n        vit_feat = self.vit_proj(vit_feat)\n        \n        # Fuse ResNet and ViT features\n        fused = torch.cat([c5, vit_feat], dim=1)\n        c5_enhanced = self.fusion(fused)\n        \n        # FPN\n        fpn_features = self.fpn([c2, c3, c4, c5_enhanced])\n        \n        # Multi-scale predictions\n        outputs = []\n        for i, (feat, head) in enumerate(zip(fpn_features, self.detection_heads)):\n            cls_scores, bbox_preds, landmark_preds = head(feat)\n            outputs.append({\n                'cls_scores': cls_scores,\n                'bbox_preds': bbox_preds,\n                'landmark_preds': landmark_preds\n            })\n            \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:41:46.375151Z","iopub.execute_input":"2026-01-20T22:41:46.375549Z","iopub.status.idle":"2026-01-20T22:41:46.385375Z","shell.execute_reply.started":"2026-01-20T22:41:46.375523Z","shell.execute_reply":"2026-01-20T22:41:46.384702Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    \"\"\"Focal Loss for handling class imbalance\"\"\"\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\nclass DetectionLoss(nn.Module):\n    \"\"\"Combined loss for face detection\"\"\"\n    def __init__(self, lambda_cls=1.0, lambda_bbox=2.0, lambda_landmark=1.0):\n        super().__init__()\n        self.lambda_cls = lambda_cls\n        self.lambda_bbox = lambda_bbox\n        self.lambda_landmark = lambda_landmark\n        self.focal_loss = FocalLoss()\n        \n    def forward(self, predictions, targets):\n        \"\"\"\n        predictions: list of dicts with keys ['cls_scores', 'bbox_preds', 'landmark_preds']\n        targets: dict with keys ['cls_labels', 'bbox_targets', 'landmark_targets']\n        \"\"\"\n        total_loss = 0\n        losses = {}\n        \n        for pred in predictions:\n            # Classification loss\n            cls_loss = self.focal_loss(\n                pred['cls_scores'].permute(0, 2, 3, 1).reshape(-1, 2),\n                targets['cls_labels'].reshape(-1, 2)\n            )\n            \n            # Bbox regression loss (smooth L1)\n            pos_mask = targets['cls_labels'][:, :, :, :, 1] > 0.5\n            if pos_mask.sum() > 0:\n                bbox_loss = F.smooth_l1_loss(\n                    pred['bbox_preds'].permute(0, 2, 3, 1)[pos_mask],\n                    targets['bbox_targets'][pos_mask],\n                    reduction='mean'\n                )\n                \n                # Landmark loss\n                landmark_loss = F.smooth_l1_loss(\n                    pred['landmark_preds'].permute(0, 2, 3, 1)[pos_mask],\n                    targets['landmark_targets'][pos_mask],\n                    reduction='mean'\n                )\n            else:\n                bbox_loss = torch.tensor(0.0, device=pred['bbox_preds'].device)\n                landmark_loss = torch.tensor(0.0, device=pred['landmark_preds'].device)\n            \n            total_loss += (self.lambda_cls * cls_loss + \n                          self.lambda_bbox * bbox_loss + \n                          self.lambda_landmark * landmark_loss)\n        \n        losses['cls_loss'] = cls_loss\n        losses['bbox_loss'] = bbox_loss\n        losses['landmark_loss'] = landmark_loss\n        losses['total_loss'] = total_loss\n        \n        return total_loss, losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:42:17.458727Z","iopub.execute_input":"2026-01-20T22:42:17.459523Z","iopub.status.idle":"2026-01-20T22:42:17.469135Z","shell.execute_reply.started":"2026-01-20T22:42:17.459491Z","shell.execute_reply":"2026-01-20T22:42:17.468266Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\n\nclass MixUp:\n    \"\"\"MixUp augmentation\"\"\"\n    def __init__(self, alpha=0.2):\n        self.alpha = alpha\n        \n    def __call__(self, batch):\n        if self.alpha > 0:\n            lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            lam = 1\n            \n        batch_size = batch['image'].size(0)\n        index = torch.randperm(batch_size).to(batch['image'].device)\n        \n        mixed_image = lam * batch['image'] + (1 - lam) * batch['image'][index]\n        batch['image'] = mixed_image\n        batch['lam'] = lam\n        batch['index'] = index\n        \n        return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:42:44.175913Z","iopub.execute_input":"2026-01-20T22:42:44.176737Z","iopub.status.idle":"2026-01-20T22:42:44.182411Z","shell.execute_reply.started":"2026-01-20T22:42:44.176704Z","shell.execute_reply":"2026-01-20T22:42:44.181611Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class FaceDetectionDataset(Dataset):\n    \"\"\"Dataset for face detection with WIDER FACE format\"\"\"\n    def __init__(self, img_dir, annotations_file, transform=None, augment=True):\n        self.img_dir = Path(img_dir)\n        self.annotations = self._load_annotations(annotations_file)\n        self.augment = augment\n        \n        # Augmentations\n        if augment:\n            self.transform = T.Compose([\n                T.ToPILImage(),\n                T.RandomHorizontalFlip(p=0.5),\n                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n                T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                T.RandomErasing(p=0.2)\n            ])\n        else:\n            self.transform = T.Compose([\n                T.ToPILImage(),\n                T.Resize((224, 224)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n    \n    def _load_annotations(self, annotations_file):\n        \"\"\"Load annotations from file\"\"\"\n        # Placeholder - implement based on your dataset format\n        # Expected format: list of dicts with 'image_path', 'bboxes', 'landmarks'\n        annotations = []\n        # Load your annotations here\n        return annotations\n    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        \n        # Load image\n        img = cv2.imread(str(self.img_dir / ann['image_path']))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Apply transforms\n        img_tensor = self.transform(img)\n        \n        # Prepare targets (simplified - implement anchor matching)\n        targets = self._prepare_targets(ann, img.shape)\n        \n        return {\n            'image': img_tensor,\n            'targets': targets\n        }\n    \n    def _prepare_targets(self, ann, img_shape):\n        \"\"\"Convert annotations to model targets\"\"\"\n        # Implement anchor matching and target preparation\n        # This is a simplified placeholder\n        targets = {\n            'cls_labels': torch.zeros((3, 56, 56, 2)),  # Placeholder\n            'bbox_targets': torch.zeros((3, 56, 56, 4)),\n            'landmark_targets': torch.zeros((3, 56, 56, 10))\n        }\n        return targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:44:07.002620Z","iopub.execute_input":"2026-01-20T22:44:07.003494Z","iopub.status.idle":"2026-01-20T22:44:07.012594Z","shell.execute_reply.started":"2026-01-20T22:44:07.003457Z","shell.execute_reply":"2026-01-20T22:44:07.011904Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# TRAINING UTILITIES\n# ============================================================================\n\ndef create_differential_optimizer(model, lr_backbone=1e-5, lr_vit=1e-4, lr_head=1e-3):\n    \"\"\"Create optimizer with differential learning rates\"\"\"\n    param_groups = [\n        # ResNet backbone - lowest LR\n        {'params': [p for n, p in model.named_parameters() \n                   if 'conv1' in n or 'layer' in n and 'vit' not in n],\n         'lr': lr_backbone, 'name': 'backbone'},\n        \n        # ViT - medium LR\n        {'params': [p for n, p in model.named_parameters() if 'vit' in n],\n         'lr': lr_vit, 'name': 'vit'},\n        \n        # Detection heads and FPN - highest LR\n        {'params': [p for n, p in model.named_parameters() \n                   if 'fpn' in n or 'detection_heads' in n or 'fusion' in n],\n         'lr': lr_head, 'name': 'head'}\n    ]\n    \n    optimizer = AdamW(param_groups, weight_decay=0.0001)\n    return optimizer\n\ndef train_epoch(model, train_loader, optimizer, criterion, device, scaler, mixup=None):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, batch in enumerate(train_loader):\n        images = batch['image'].to(device)\n        targets = {k: v.to(device) for k, v in batch['targets'].items()}\n        \n        # Apply MixUp\n        if mixup:\n            batch_data = {'image': images, **targets}\n            batch_data = mixup(batch_data)\n            images = batch_data['image']\n        \n        optimizer.zero_grad()\n        \n        # Mixed precision training\n        with autocast():\n            predictions = model(images)\n            loss, loss_dict = criterion(predictions, targets)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n    \n    return total_loss / len(train_loader)\n\n@torch.no_grad()\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate model\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    for batch in val_loader:\n        images = batch['image'].to(device)\n        targets = {k: v.to(device) for k, v in batch['targets'].items()}\n        \n        predictions = model(images)\n        loss, _ = criterion(predictions, targets)\n        total_loss += loss.item()\n    \n    return total_loss / len(val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:44:58.451365Z","iopub.execute_input":"2026-01-20T22:44:58.451686Z","iopub.status.idle":"2026-01-20T22:44:58.462410Z","shell.execute_reply.started":"2026-01-20T22:44:58.451662Z","shell.execute_reply":"2026-01-20T22:44:58.461620Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):\n    \"\"\"Complete training loop\"\"\"\n    model = model.to(device)\n    \n    # Optimizer with differential learning rates\n    optimizer = create_differential_optimizer(model)\n    \n    # Scheduler\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n    \n    # Loss function\n    criterion = DetectionLoss()\n    \n    # Mixed precision scaler\n    scaler = GradScaler()\n    \n    # MixUp augmentation\n    mixup = MixUp(alpha=0.2)\n    \n    best_val_loss = float('inf')\n    patience = 10\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n        \n        train_loss = train_epoch(model, train_loader, optimizer, criterion, \n                                device, scaler, mixup)\n        val_loss = validate(model, val_loader, criterion, device)\n        \n        scheduler.step()\n        \n        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, 'best_model.pth')\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print('Early stopping triggered')\n            break\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:45:39.451012Z","iopub.execute_input":"2026-01-20T22:45:39.451574Z","iopub.status.idle":"2026-01-20T22:45:39.458558Z","shell.execute_reply.started":"2026-01-20T22:45:39.451538Z","shell.execute_reply":"2026-01-20T22:45:39.457870Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class FaceDetectorInference:\n    \"\"\"Optimized inference class for deployment\"\"\"\n    def __init__(self, model_path, device='cuda', use_fp16=True, conf_threshold=0.7):\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        self.conf_threshold = conf_threshold\n        self.use_fp16 = use_fp16 and device == 'cuda'\n        \n        # Load model\n        self.model = HybridFaceDetector(pretrained=False)\n        checkpoint = torch.load(model_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Convert to FP16 for faster inference\n        if self.use_fp16:\n            self.model = self.model.half()\n        \n        # Image preprocessing\n        self.transform = T.Compose([\n            T.ToPILImage(),\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Warm up\n        self._warmup()\n    \n    def _warmup(self):\n        \"\"\"Warm up model for faster initial inference\"\"\"\n        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)\n        if self.use_fp16:\n            dummy_input = dummy_input.half()\n        with torch.no_grad():\n            _ = self.model(dummy_input)\n    \n    @torch.no_grad()\n    def detect_faces(self, frame):\n        \"\"\"\n        Detect faces in a single frame\n        Args:\n            frame: numpy array (H, W, 3) in BGR format\n        Returns:\n            detections: list of dicts with 'bbox', 'landmarks', 'confidence'\n        \"\"\"\n        # Preprocess\n        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        img_tensor = self.transform(img_rgb).unsqueeze(0).to(self.device)\n        \n        if self.use_fp16:\n            img_tensor = img_tensor.half()\n        \n        # Inference\n        predictions = self.model(img_tensor)\n        \n        # Post-process predictions\n        detections = self._postprocess(predictions, frame.shape[:2])\n        \n        return detections\n    \n    def _postprocess(self, predictions, orig_shape):\n        \"\"\"Convert model outputs to detections\"\"\"\n        detections = []\n        \n        # Simplified post-processing (implement full NMS and decoding)\n        for level_pred in predictions:\n            cls_scores = torch.sigmoid(level_pred['cls_scores'])\n            \n            # Get face confidence scores\n            face_scores = cls_scores[:, 1::2, :, :]  # Face class\n            \n            # Threshold\n            mask = face_scores > self.conf_threshold\n            \n            if mask.sum() > 0:\n                # Extract detections (simplified)\n                # Implement full anchor decoding and NMS here\n                pass\n        \n        return detections\n    \n    def visualize_detections(self, frame, detections):\n        \"\"\"Draw detections on frame\"\"\"\n        for det in detections:\n            bbox = det['bbox']\n            conf = det['confidence']\n            landmarks = det.get('landmarks', None)\n            \n            # Draw bounding box\n            cv2.rectangle(frame, \n                         (int(bbox[0]), int(bbox[1])), \n                         (int(bbox[2]), int(bbox[3])), \n                         (0, 255, 0), 2)\n            \n            # Draw confidence\n            cv2.putText(frame, f'{conf:.2f}', \n                       (int(bbox[0]), int(bbox[1])-10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n            \n            # Draw landmarks\n            if landmarks is not None:\n                for lm in landmarks:\n                    cv2.circle(frame, (int(lm[0]), int(lm[1])), 2, (255, 0, 0), -1)\n        \n        return frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:46:35.462124Z","iopub.execute_input":"2026-01-20T22:46:35.462428Z","iopub.status.idle":"2026-01-20T22:46:35.477571Z","shell.execute_reply.started":"2026-01-20T22:46:35.462401Z","shell.execute_reply":"2026-01-20T22:46:35.476834Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def webcam_demo(model_path, skip_frames=2):\n    \"\"\"Real-time webcam face detection demo\"\"\"\n    detector = FaceDetectorInference(model_path, device='cuda')\n    \n    cap = cv2.VideoCapture(0)\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n    \n    frame_count = 0\n    \n    print(\"Press 'q' to quit\")\n    \n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Frame skipping for performance\n        if frame_count % skip_frames == 0:\n            detections = detector.detect_faces(frame)\n            frame = detector.visualize_detections(frame, detections)\n        \n        cv2.imshow('Face Detection', frame)\n        frame_count += 1\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:47:14.460893Z","iopub.execute_input":"2026-01-20T22:47:14.461193Z","iopub.status.idle":"2026-01-20T22:47:14.467393Z","shell.execute_reply.started":"2026-01-20T22:47:14.461168Z","shell.execute_reply":"2026-01-20T22:47:14.466455Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Example: Initialize model\n    model = HybridFaceDetector(pretrained=True)\n    print(f'Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T22:47:33.935486Z","iopub.execute_input":"2026-01-20T22:47:33.935795Z","iopub.status.idle":"2026-01-20T22:47:35.731731Z","shell.execute_reply.started":"2026-01-20T22:47:33.935757Z","shell.execute_reply":"2026-01-20T22:47:35.730939Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 188MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model parameters: 95.57M\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torchvision.transforms as T\nfrom pathlib import Path\nimport cv2\nimport numpy as np\n\n# Ye WebFaceDataset class mein add karo (jo tumne define kiya hai):\n\nclass WebFaceDataset(Dataset):\n    def __init__(self, data_dir, augment=True, img_size=224, max_samples=None):\n        # ... existing code ...\n        print(f'✓ Found {len(self.image_paths)} images')\n        \n        # Augmentations (add this part)\n        if augment:\n            self.transform = T.Compose([\n                T.ToPILImage(),\n                T.Resize((img_size, img_size)),\n                T.RandomHorizontalFlip(p=0.5),\n                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n                T.RandomRotation(15),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                T.RandomErasing(p=0.2, scale=(0.02, 0.15))\n            ])\n        else:\n            self.transform = T.Compose([\n                T.ToPILImage(),\n                T.Resize((img_size, img_size)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])\n    \n    def __len__(self):  # ⬅️ YE ADD KARO\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):  # ⬅️ YE BHI ADD KARO\n        img_path = self.image_paths[idx]\n        img = cv2.imread(str(img_path))\n        if img is None:\n            img = np.zeros((112, 112, 3), dtype=np.uint8)\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        img_tensor = self.transform(img)\n        targets = self._create_synthetic_targets()\n        \n        return {'image': img_tensor, 'targets': targets, 'path': str(img_path)}\n    \n    def _create_synthetic_targets(self):  # ⬅️ YE BHI\n        targets = {\n            'cls_labels': torch.zeros((4, 3, 56, 56, 2)),\n            'bbox_targets': torch.zeros((4, 3, 56, 56, 4)),\n            'landmark_targets': torch.zeros((4, 3, 56, 56, 10))\n        }\n        \n        for level in range(4):\n            scale = 2 ** level\n            h, w = 56 // scale, 56 // scale\n            center_y, center_x = h // 2, w // 2\n            \n            targets['cls_labels'][level, :, center_y-2:center_y+2, center_x-2:center_x+2, 1] = 1.0\n            targets['bbox_targets'][level, :, center_y-2:center_y+2, center_x-2:center_x+2] = torch.tensor([0.1, 0.1, 0.9, 0.9])\n            \n            landmarks = torch.tensor([0.35, 0.40, 0.65, 0.40, 0.50, 0.60, 0.40, 0.75, 0.60, 0.75])\n            targets['landmark_targets'][level, :, center_y-2:center_y+2, center_x-2:center_x+2] = landmarks\n        \n        return targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:05:27.956703Z","iopub.execute_input":"2026-01-20T23:05:27.957030Z","iopub.status.idle":"2026-01-20T23:05:27.970785Z","shell.execute_reply.started":"2026-01-20T23:05:27.957007Z","shell.execute_reply":"2026-01-20T23:05:27.969950Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Artifact se COMPLETE code copy karo ek cell mein\n# Phir neeche wala training code run karo\n\n# Ya direct ye updated class copy karo:\n\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nimport cv2\nimport numpy as np\n\nclass WebFaceDataset(Dataset):\n    def __init__(self, data_dir, augment=True, img_size=224, max_samples=None):\n        self.data_dir = Path(data_dir)\n        self.augment = augment\n        self.img_size = img_size\n        \n        print(f'Loading dataset from: {data_dir}')\n        self.image_paths = []\n        \n        if not self.data_dir.exists():\n            raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n        \n        for person_dir in sorted(self.data_dir.iterdir()):\n            if person_dir.is_dir():\n                img_files = list(person_dir.glob('*.jpg')) + list(person_dir.glob('*.png'))\n                self.image_paths.extend(img_files)\n                \n                if max_samples and len(self.image_paths) >= max_samples:\n                    self.image_paths = self.image_paths[:max_samples]\n                    break\n        \n        if len(self.image_paths) == 0:\n            raise ValueError(f\"No images found in {data_dir}\")\n        \n        print(f'✓ Found {len(self.image_paths)} images')\n        \n        # Rest of the class remains same...\n        # (transform, __len__, __getitem__, _create_synthetic_targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:05:33.708810Z","iopub.execute_input":"2026-01-20T23:05:33.709120Z","iopub.status.idle":"2026-01-20T23:05:33.716799Z","shell.execute_reply.started":"2026-01-20T23:05:33.709096Z","shell.execute_reply":"2026-01-20T23:05:33.716012Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# DATA_DIR update karo\nDATA_DIR = '/kaggle/input/webface-112x112/webface_112x112'  # Ek level neeche\n\n# Ya phir auto-detect karo:\nimport os\n\nBASE_DIR = '/kaggle/input/webface-112x112'\nsubdirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\nDATA_DIR = os.path.join(BASE_DIR, subdirs[0])  # webface_112x112\n\nprint(f'Actual data directory: {DATA_DIR}')\n\n# Ab dataset banao\nfull_dataset = WebFaceDataset(DATA_DIR, augment=True, img_size=224)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:05:36.679543Z","iopub.execute_input":"2026-01-20T23:05:36.679909Z","iopub.status.idle":"2026-01-20T23:05:55.576675Z","shell.execute_reply.started":"2026-01-20T23:05:36.679882Z","shell.execute_reply":"2026-01-20T23:05:55.576003Z"}},"outputs":[{"name":"stdout","text":"Actual data directory: /kaggle/input/webface-112x112/webface_112x112\nLoading dataset from: /kaggle/input/webface-112x112/webface_112x112\n✓ Found 490623 images\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.cuda.amp import autocast, GradScaler\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport numpy as np\nimport cv2\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ────────────────────────────────────────────────────────────────\n#  ANCHOR GENERATOR + MATCHER\n# ────────────────────────────────────────────────────────────────\nclass AnchorGenerator:\n    def __init__(self, strides=[8,16,32,64], anchor_scales=[16,32,64,128], ratios=[0.5,1,2]):\n        self.strides      = strides\n        self.anchor_scales = anchor_scales\n        self.ratios       = ratios\n        self.num_anchors  = len(anchor_scales) * len(ratios)\n\n    def generate_anchors_per_level(self, h, w, stride, img_size=224):\n        anchors = []\n        for i in range(h):\n            for j in range(w):\n                cx = (j + 0.5) * stride\n                cy = (i + 0.5) * stride\n                for s in self.anchor_scales:\n                    for r in self.ratios:\n                        w_a = s * r**0.5\n                        h_a = s / r**0.5\n                        x1 = max(0, cx - w_a/2)\n                        y1 = max(0, cy - h_a/2)\n                        x2 = min(img_size, cx + w_a/2)\n                        y2 = min(img_size, cy + h_a/2)\n                        anchors.append([x1,y1,x2,y2])\n        return np.array(anchors)\n\n    def generate_all(self, img_size=224):\n        all_anchors = []\n        for stride in self.strides:\n            fs = img_size // stride\n            anchors = self.generate_anchors_per_level(fs, fs, stride, img_size)\n            all_anchors.append(anchors)\n        return all_anchors\n\n    @staticmethod\n    def iou(box1, box2):\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n        inter = max(0, x2-x1) * max(0, y2-y1)\n        area1 = (box1[2]-box1[0]) * (box1[3]-box1[1])\n        area2 = (box2[2]-box2[0]) * (box2[3]-box2[1])\n        return inter / (area1 + area2 - inter + 1e-6)\n\n    def match(self, anchors_per_level, gt_boxes, iou_thresh=0.5, img_size=224):\n        if len(gt_boxes) == 0:\n            cls  = np.full((len(anchors_per_level), 2), [1.0, 0.0])\n            box  = np.zeros((len(anchors_per_level), 4))\n            lms  = np.zeros((len(anchors_per_level), 10))\n            return cls, box, lms\n\n        cls  = np.full((len(anchors_per_level), 2), [1.0, 0.0])\n        box  = np.zeros((len(anchors_per_level), 4))\n        lms  = np.zeros((len(anchors_per_level), 10))\n\n        for i, a in enumerate(anchors_per_level):\n            best_iou = 0\n            best_j   = -1\n            for j, g in enumerate(gt_boxes):\n                iou = self.iou(a, g[:4])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j   = j\n\n            if best_iou >= iou_thresh:\n                cls[i] = [0.0, 1.0]\n                gt     = gt_boxes[best_j]\n                ax,ay,aw,ah = (a[0]+a[2])/2, (a[1]+a[3])/2, a[2]-a[0], a[3]-a[1]\n                gx,gy,gw,gh = (gt[0]+gt[2])/2, (gt[1]+gt[3])/2, gt[2]-gt[0], gt[3]-gt[1]\n\n                box[i,0] = (gx - ax) / (aw + 1e-6)\n                box[i,1] = (gy - ay) / (ah + 1e-6)\n                box[i,2] = np.log(gw / (aw + 1e-6) + 1e-6)\n                box[i,3] = np.log(gh / (ah + 1e-6) + 1e-6)\n\n                if len(gt) >= 14:\n                    lms[i] = gt[4:14] / img_size\n\n        return cls, box, lms\n\n# ────────────────────────────────────────────────────────────────\n#  DATASET (synthetic centered boxes)\n# ────────────────────────────────────────────────────────────────\nclass WebFaceDataset(Dataset):\n    def __init__(self, root, img_size=224, max_samples=None):\n        self.root     = Path(root)\n        self.img_size = img_size\n        self.transform = T.Compose([\n            T.ToPILImage(),\n            T.Resize((img_size, img_size)),\n            T.RandomHorizontalFlip(),\n            T.ColorJitter(0.15,0.15,0.15),\n            T.ToTensor(),\n            T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n        ])\n\n        self.paths = []\n        for d in self.root.iterdir():\n            if d.is_dir():\n                self.paths.extend(list(d.glob(\"*.jpg\")) + list(d.glob(\"*.png\")))\n\n        if max_samples:\n            self.paths = self.paths[:max_samples]\n\n        print(f\"Found {len(self.paths)} images\")\n\n        self.anchor_gen = AnchorGenerator()\n        self.all_anchors = self.anchor_gen.generate_all(img_size)\n\n        self.annotations = {}\n        self._fake_annotations()\n\n    def _fake_annotations(self):\n        s = 112\n        m = int(s * 0.12)\n        box = [m, m, s-m, s-m]\n        lms = [0.35,0.4, 0.65,0.4, 0.5,0.6, 0.4,0.75, 0.6,0.75]\n        lms = [v*s for v in lms]\n        for p in self.paths:\n            self.annotations[str(p)] = {'box': box + lms, 'orig': s}\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        p   = str(self.paths[idx])\n        img = cv2.imread(p)\n        if img is None:\n            img = np.zeros((112,112,3), np.uint8)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        anno = self.annotations[p]\n        gt   = np.array([anno['box']])   # [1,14]\n        scale = self.img_size / anno['orig']\n        gt[:,:4]  *= scale\n        gt[:,4:]  *= scale\n\n        targets = {'cls':[], 'box':[], 'lm':[]}\n        for anchors in self.all_anchors:\n            c,b,l = self.anchor_gen.match(anchors, gt, img_size=self.img_size)\n            n = self.anchor_gen.num_anchors\n            h = w = int(np.sqrt(len(anchors) / n))\n            c = c.reshape(h,w,n,2).transpose(2,0,1,3)   # n,h,w,2\n            b = b.reshape(h,w,n,4).transpose(2,0,1,3)\n            l = l.reshape(h,w,n,10).transpose(2,0,1,3)\n            targets['cls'].append(torch.from_numpy(c).float())\n            targets['box'].append(torch.from_numpy(b).float())\n            targets['lm'].append( torch.from_numpy(l).float())\n\n        return {'img': self.transform(img), 'targets': targets}\n\n# ────────────────────────────────────────────────────────────────\n#  LIGHT FPN + DET HEAD\n# ────────────────────────────────────────────────────────────────\nclass LightFPN(nn.Module):\n    def __init__(self, in_ch=[512,1024,2048], out=256):\n        super().__init__()\n        self.lat = nn.ModuleList([nn.Conv2d(c, out, 1) for c in in_ch])\n        self.smooth = nn.ModuleList([nn.Conv2d(out, out, 3, padding=1) for _ in range(3)])\n\n    def forward(self, feats):  # C3,C4,C5\n        p5   = self.lat[2](feats[2])\n        p4   = self.lat[1](feats[1]) + F.interpolate(p5, scale_factor=2, mode='nearest')\n        p3   = self.lat[0](feats[0]) + F.interpolate(p4, scale_factor=2, mode='nearest')\n        p3   = self.smooth[0](p3)\n        p4   = self.smooth[1](p4)\n        p5   = self.smooth[2](p5)\n        return [p3,p4,p5]\n\nclass FaceDetector(nn.Module):\n    def __init__(self, num_anchors=9):\n        super().__init__()\n        rn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.backbone = nn.Sequential(\n            rn.conv1, rn.bn1, rn.relu, rn.maxpool,\n            rn.layer1,  # C2  56×56×256\n            rn.layer2,  # C3  28×28×512\n            rn.layer3,  # C4  14×14×1024\n            rn.layer4   # C5   7×7×2048\n        )\n\n        self.fpn = LightFPN()\n\n        self.cls_head  = nn.Conv2d(256, num_anchors*2,  1)\n        self.box_head  = nn.Conv2d(256, num_anchors*4,  1)\n        self.lm_head   = nn.Conv2d(256, num_anchors*10, 1)\n\n    def forward(self, x):\n        _, c3, c4, c5 = self.backbone(x)               # skip C2 for speed\n        p3,p4,p5 = self.fpn([c3,c4,c5])\n\n        outs = []\n        for p in [p3,p4,p5]:\n            outs.append({\n                'cls': self.cls_head(p),\n                'box': self.box_head(p),\n                'lm':  self.lm_head(p)\n            })\n        return outs\n\n# ────────────────────────────────────────────────────────────────\n#  LOSS\n# ────────────────────────────────────────────────────────────────\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=0.25):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, pred, tgt):\n        bce = F.binary_cross_entropy_with_logits(pred, tgt, reduction='none')\n        pt  = torch.exp(-bce)\n        loss = self.alpha * (1-pt)**self.gamma * bce\n        return loss.mean()\n\nclass DetectionLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.focal = FocalLoss()\n\n    def forward(self, preds, targets):\n        cls_loss = box_loss = lm_loss = 0.0\n        n_pos = 1e-6\n\n        for pred, t_cls, t_box, t_lm in zip(preds, targets['cls'], targets['box'], targets['lm']):\n            B,_,H,W = pred['cls'].shape\n            pred_cls = pred['cls'].view(B,9,2,H,W).permute(0,1,3,4,2)\n            pred_box = pred['box'].view(B,9,4,H,W).permute(0,1,3,4,2)\n            pred_lm  = pred['lm'].view( B,9,10,H,W).permute(0,1,3,4,2)\n\n            cls_tgt = t_cls.view(B,9,2,H,W).permute(0,1,3,4,2)\n\n            cls_l = self.focal(pred_cls.reshape(-1,2), cls_tgt.reshape(-1,2))\n\n            pos = cls_tgt[...,1] > 0.5\n            n = pos.sum() + 1e-6\n            n_pos += n\n\n            if n > 0:\n                box_l = F.smooth_l1_loss(pred_box[pos], t_box[pos], reduction='sum') / n\n                lm_l  = F.smooth_l1_loss(pred_lm[pos],  t_lm[pos],  reduction='sum') / n\n            else:\n                box_l = lm_l = torch.tensor(0.0, device=pred_cls.device)\n\n            cls_loss += cls_l\n            box_loss += box_l\n            lm_loss  += lm_l\n\n        total = cls_loss + 2*box_loss + lm_loss\n        return total / len(preds), {\n            'cls': cls_loss.item()/len(preds),\n            'box': box_loss.item()/len(preds),\n            'lm':  lm_loss.item()/len(preds)\n        }\n\n# ────────────────────────────────────────────────────────────────\n#  TRAIN\n# ────────────────────────────────────────────────────────────────\ndef train(data_dir=\"/kaggle/input/webface-112x112/webface_112x112/\",\n          epochs=12, bs=32, max_samples=20000):\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Device: {device}\")\n\n    ds = WebFaceDataset(data_dir, max_samples=max_samples)\n    n   = len(ds)\n    trn = int(0.9 * n)\n    train_ds, val_ds = torch.utils.data.random_split(ds, [trn, n-trn])\n\n    train_loader = DataLoader(train_ds, bs, True,  num_workers=2, pin_memory=True)\n    val_loader   = DataLoader(val_ds,   bs, False, num_workers=2, pin_memory=True)\n\n    model = FaceDetector().to(device)\n    opt   = AdamW(model.parameters(), 1e-4, weight_decay=1e-4)\n    sched = CosineAnnealingLR(opt, epochs)\n    loss_fn = DetectionLoss()\n    scaler  = GradScaler()\n\n    save_dir = f\"face_run_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    for ep in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in train_loader:\n            img = batch['img'].to(device)\n            tgt = {k: [v.to(device) for v in vals] for k,vals in batch['targets'].items()}\n\n            opt.zero_grad()\n            with autocast():\n                out = model(img)\n                loss, stats = loss_fn(out, tgt)\n\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n            total_loss += loss.item()\n\n        print(f\"[{ep+1:2d}/{epochs}]  loss {total_loss/len(train_loader):.4f}\")\n\n        sched.step()\n\n        if (ep+1) % 4 == 0:\n            torch.save(model.state_dict(), f\"{save_dir}/ep{ep+1}.pth\")\n\n    print(\"Done.\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T00:04:04.077028Z","iopub.execute_input":"2026-01-21T00:04:04.077379Z","iopub.status.idle":"2026-01-21T00:04:04.125534Z","shell.execute_reply.started":"2026-01-21T00:04:04.077345Z","shell.execute_reply":"2026-01-21T00:04:04.124691Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"\"\"\"\nSIMPLE RESNET + VIT FACE DETECTION - NO COMPLEX FPN\n✅ Clean and working\n✅ For Kaggle: /kaggle/input/webface-112x112/webface_112x112\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.cuda.amp import autocast, GradScaler\nimport torchvision.transforms as T\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nimport numpy as np\nimport cv2\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"✅ Starting...\")\n\n# ============================================================================\n# VISION TRANSFORMER\n# ============================================================================\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, img_size=7, patch_size=1, in_chans=2048, embed_dim=512):\n        super().__init__()\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        \n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim=512, num_heads=8):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim),\n        )\n        \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass SimpleViT(nn.Module):\n    def __init__(self, img_size=7, in_chans=2048, embed_dim=512, depth=4, num_heads=8):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size, 1, in_chans, embed_dim)\n        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x = self.patch_embed(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x.mean(dim=1)  # Global average pooling\n\n# ============================================================================\n# SIMPLE MODEL\n# ============================================================================\n\nclass ResNetViTFaceDetector(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # ResNet backbone\n        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.backbone = nn.Sequential(\n            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n        )\n        \n        # ViT on top\n        self.vit = SimpleViT(img_size=7, in_chans=2048, embed_dim=512, depth=4)\n        \n        # Simple detection heads\n        self.bbox_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 4)  # x, y, w, h\n        )\n        \n        self.cls_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)  # background, face\n        )\n        \n        self.landmark_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 10)  # 5 landmarks × 2 coords\n        )\n        \n    def forward(self, x):\n        feat = self.backbone(x)  # [B, 2048, 7, 7]\n        feat_vit = self.vit(feat)  # [B, 512]\n        \n        bbox = self.bbox_head(feat_vit)\n        cls = self.cls_head(feat_vit)\n        landmarks = self.landmark_head(feat_vit)\n        \n        return {\n            'bbox': bbox,\n            'cls': cls,\n            'landmarks': landmarks\n        }\n\n# ============================================================================\n# DATASET\n# ============================================================================\n\nclass WebFaceDataset(Dataset):\n    def __init__(self, data_dir, img_size=224, max_samples=None):\n        self.data_dir = Path(data_dir)\n        self.img_size = img_size\n        \n        print(f'\\n📂 Loading from: {data_dir}')\n        \n        self.image_paths = []\n        for person_dir in sorted(self.data_dir.iterdir()):\n            if person_dir.is_dir():\n                imgs = list(person_dir.glob('*.jpg')) + list(person_dir.glob('*.png'))\n                self.image_paths.extend(imgs)\n                \n                if max_samples and len(self.image_paths) >= max_samples:\n                    self.image_paths = self.image_paths[:max_samples]\n                    break\n        \n        print(f'✅ Found {len(self.image_paths):,} images\\n')\n        \n        self.transform = T.Compose([\n            T.ToPILImage(),\n            T.Resize((img_size, img_size)),\n            T.RandomHorizontalFlip(p=0.5),\n            T.ColorJitter(brightness=0.2, contrast=0.2),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = str(self.image_paths[idx])\n        img = cv2.imread(img_path)\n        \n        if img is None:\n            img = np.zeros((112, 112, 3), dtype=np.uint8)\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        img_tensor = self.transform(img)\n        \n        # Simple centered face target (normalized 0-1)\n        target = {\n            'bbox': torch.tensor([0.1, 0.1, 0.8, 0.8], dtype=torch.float32),  # x, y, w, h\n            'cls': torch.tensor([0.0, 1.0], dtype=torch.float32),  # background, face\n            'landmarks': torch.tensor([0.35, 0.4, 0.65, 0.4, 0.5, 0.6, 0.4, 0.75, 0.6, 0.75], dtype=torch.float32)\n        }\n        \n        return img_tensor, target\n\n# ============================================================================\n# LOSS\n# ============================================================================\n\nclass SimpleLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        \n    def forward(self, pred, target):\n        cls_loss = self.bce(pred['cls'], target['cls'])\n        bbox_loss = F.smooth_l1_loss(pred['bbox'], target['bbox'])\n        landmark_loss = F.smooth_l1_loss(pred['landmarks'], target['landmarks'])\n        \n        total_loss = cls_loss + 2.0 * bbox_loss + landmark_loss\n        \n        return total_loss, {\n            'cls': cls_loss.item(),\n            'bbox': bbox_loss.item(),\n            'landmark': landmark_loss.item(),\n            'total': total_loss.item()\n        }\n\n# ============================================================================\n# TRAINING\n# ============================================================================\n\ndef train():\n    DATA_DIR = '/kaggle/input/webface-112x112/webface_112x112/'\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    num_epochs = 30\n    batch_size = 64\n    max_samples = 20000\n    \n    print(f'{\"#\"*60}')\n    print(f'# Device: {device}')\n    print(f'# Epochs: {num_epochs}')\n    print(f'# Batch: {batch_size}')\n    print(f'{\"#\"*60}')\n    \n    # Dataset\n    dataset = WebFaceDataset(DATA_DIR, max_samples=max_samples)\n    \n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    \n    print(f'Train: {train_size:,} | Val: {val_size:,}\\n')\n    \n    # Model\n    model = ResNetViTFaceDetector().to(device)\n    optimizer = AdamW(model.parameters(), lr=1e-4)\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n    criterion = SimpleLoss()\n    scaler = GradScaler()\n    \n    save_dir = f'./output_{datetime.now().strftime(\"%H%M%S\")}'\n    os.makedirs(save_dir, exist_ok=True)\n    \n    best_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        print(f'\\n{\"=\"*60}')\n        print(f'EPOCH {epoch+1}/{num_epochs}')\n        print(f'{\"=\"*60}')\n        \n        # TRAIN\n        model.train()\n        train_loss = 0\n        \n        for batch_idx, (images, targets) in enumerate(train_loader):\n            images = images.to(device)\n            targets = {k: v.to(device) for k, v in targets.items()}\n            \n            optimizer.zero_grad()\n            \n            with autocast():\n                pred = model(images)\n                loss, loss_dict = criterion(pred, targets)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_loss += loss.item()\n            \n            if batch_idx % 20 == 0:\n                print(f'  [{batch_idx:3d}/{len(train_loader)}] '\n                      f'Loss: {loss.item():.4f} | '\n                      f'Cls: {loss_dict[\"cls\"]:.4f} | '\n                      f'Bbox: {loss_dict[\"bbox\"]:.4f}')\n        \n        # VAL\n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for images, targets in val_loader:\n                images = images.to(device)\n                targets = {k: v.to(device) for k, v in targets.items()}\n                \n                pred = model(images)\n                loss, _ = criterion(pred, targets)\n                val_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        \n        print(f'\\n  TRAIN Loss: {train_loss:.4f}')\n        print(f'  VAL   Loss: {val_loss:.4f}')\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), f'{save_dir}/best.pth')\n            print(f'  ✅ Best model saved! Loss: {best_loss:.4f}')\n        \n        scheduler.step()\n    \n    print(f'\\n{\"#\"*60}')\n    print(f'# ✅ DONE! Best Loss: {best_loss:.4f}')\n    print(f'# Saved: {save_dir}/best.pth')\n    print(f'{\"#\"*60}\\n')\n\n# ============================================================================\n# RUN\n# ============================================================================\n\nif __name__ == '__main__':\n    train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T00:10:42.308925Z","iopub.execute_input":"2026-01-21T00:10:42.309309Z","iopub.status.idle":"2026-01-21T00:10:46.085744Z","shell.execute_reply.started":"2026-01-21T00:10:42.309274Z","shell.execute_reply":"2026-01-21T00:10:46.084330Z"}},"outputs":[{"name":"stdout","text":"✅ Starting...\n############################################################\n# Device: cuda\n# Epochs: 30\n# Batch: 64\n############################################################\n\n📂 Loading from: /kaggle/input/webface-112x112/webface_112x112/\n✅ Found 20,000 images\n\nTrain: 18,000 | Val: 2,000\n\n\n============================================================\nEPOCH 1/30\n============================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3792082027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_55/3792082027.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3792082027.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 2048, 7, 7]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mfeat_vit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 39.12 MiB is free. Process 6704 has 15.85 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 32.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 39.12 MiB is free. Process 6704 has 15.85 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 32.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":61},{"cell_type":"code","source":"# after training\nmodel.load_state_dict(torch.load(/kaggle/input/webface-112x112/webface_112x112))\nimg, _ = ds[some_index]           # or load your own test image\ndets = inference(model, img)\nshow_result(img, dets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T00:05:24.204583Z","iopub.execute_input":"2026-01-21T00:05:24.205164Z","iopub.status.idle":"2026-01-21T00:05:24.210879Z","shell.execute_reply.started":"2026-01-21T00:05:24.205135Z","shell.execute_reply":"2026-01-21T00:05:24.209822Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_55/2970396557.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model.load_state_dict(torch.load(/kaggle/input/webface-112x112/webface_112x112))\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"],"ename":"SyntaxError","evalue":"invalid decimal literal (2970396557.py, line 2)","output_type":"error"}],"execution_count":58},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# Auto-detect data directory\nBASE_DIR = '/kaggle/input/webface-112x112'\nsubdirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\nDATA_DIR = os.path.join(BASE_DIR, subdirs[0])\n\nprint(f'✓ Data directory: {DATA_DIR}')\n\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nDEVICE = 'cuda'\nSAVE_DIR = './checkpoints'\n\n# Create dataset\nprint('\\nLoading dataset...')\nfull_dataset = WebFaceDataset(DATA_DIR, augment=True, img_size=224)\n\ntotal_size = len(full_dataset)\ntrain_size = int(0.9 * total_size)\n\nindices = np.arange(total_size)\nnp.random.shuffle(indices)\n\ntrain_dataset = Subset(full_dataset, indices[:train_size])\nval_dataset = Subset(full_dataset, indices[train_size:])\n\nprint(f'✓ Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n\n# Create dataloaders WITH custom collate_fn\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=2, \n    pin_memory=True, \n    drop_last=True,\n    collate_fn=collate_fn  # ✓ IMPORTANT!\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=2, \n    pin_memory=True,\n    collate_fn=collate_fn  # ✓ IMPORTANT!\n)\n\nprint(f'✓ Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n\n# Test batch\nprint('\\nTesting data loading...')\ntest_batch = next(iter(train_loader))\nprint(f'✓ Batch image shape: {test_batch[\"image\"].shape}')\nprint(f'✓ Targets structure: {len(test_batch[\"targets\"][\"cls_labels\"])} FPN levels')\n\n# START TRAINING\nprint('\\n' + '='*80)\nprint('🚀 STARTING TRAINING - ALL BUGS FIXED!')\nprint('='*80 + '\\n')\n\ntrained_model = train_model(\n    model, \n    train_loader, \n    val_loader, \n    num_epochs=NUM_EPOCHS, \n    device=DEVICE,\n    resume_from=None,\n    save_dir=SAVE_DIR\n)\n\nprint('\\n✅ TRAINING COMPLETE!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:19:54.827624Z","iopub.execute_input":"2026-01-20T23:19:54.828293Z","iopub.status.idle":"2026-01-20T23:30:09.306707Z","shell.execute_reply.started":"2026-01-20T23:19:54.828253Z","shell.execute_reply":"2026-01-20T23:30:09.305611Z"}},"outputs":[{"name":"stdout","text":"✓ Data directory: /kaggle/input/webface-112x112/webface_112x112\n\nLoading dataset...\nLoading dataset from: /kaggle/input/webface-112x112/webface_112x112\n✓ Found 490623 images\n✓ Train: 441560, Val: 49063\n✓ Train batches: 13798, Val batches: 1534\n\nTesting data loading...\n✓ Batch image shape: torch.Size([32, 3, 224, 224])\n✓ Targets structure: 4 FPN levels\n\n================================================================================\n🚀 STARTING TRAINING - ALL BUGS FIXED!\n================================================================================\n\n\n============================================================\nStarting training from epoch 0\n============================================================\n\n\nEpoch 1/50\n------------------------------------------------------------\nEpoch 1, Batch 0/13798, Loss: 2.4778, Cls: 0.0432, Bbox: 0.2113\nEpoch 1, Batch 100/13798, Loss: 0.0203, Cls: 0.0023, Bbox: 0.0008\nEpoch 1, Batch 200/13798, Loss: 0.0131, Cls: 0.0016, Bbox: 0.0006\nEpoch 1, Batch 300/13798, Loss: 0.0170, Cls: 0.0012, Bbox: 0.0012\nEpoch 1, Batch 400/13798, Loss: 0.0096, Cls: 0.0012, Bbox: 0.0005\nEpoch 1, Batch 500/13798, Loss: 0.0044, Cls: 0.0007, Bbox: 0.0001\nEpoch 1, Batch 600/13798, Loss: 0.0058, Cls: 0.0005, Bbox: 0.0004\nEpoch 1, Batch 700/13798, Loss: 0.0055, Cls: 0.0005, Bbox: 0.0003\nEpoch 1, Batch 800/13798, Loss: 0.0066, Cls: 0.0005, Bbox: 0.0004\nEpoch 1, Batch 900/13798, Loss: 0.0042, Cls: 0.0004, Bbox: 0.0002\nEpoch 1, Batch 1000/13798, Loss: 0.0033, Cls: 0.0004, Bbox: 0.0002\nEpoch 1, Batch 1100/13798, Loss: 0.0049, Cls: 0.0004, Bbox: 0.0003\nEpoch 1, Batch 1200/13798, Loss: 0.0050, Cls: 0.0004, Bbox: 0.0003\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1525078303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m trained_model = train_model(\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/719581179.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device, resume_from, save_dir)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         train_loss = train_epoch(model, train_loader, optimizer, criterion, \n\u001b[0m\u001b[1;32m    609\u001b[0m                                 device, scaler, epoch+1)\n\u001b[1;32m    610\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/719581179.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, device, scaler, epoch)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"# ============================================================================\n# COMPLETE TRAINING SCRIPT\n# ============================================================================\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# --- 1. SETUP & CONFIGURATION ---\n# Auto-detect data directory\nBASE_DIR = '/kaggle/input/webface-112x112'\n\n# Verify directory exists before proceeding\nif os.path.exists(BASE_DIR):\n    subdirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n    # Handle case where dataset might be directly in BASE_DIR or a subdir\n    if subdirs:\n        DATA_DIR = os.path.join(BASE_DIR, subdirs[0])\n    else:\n        DATA_DIR = BASE_DIR\nelse:\n    # Fallback or raise error if path is wrong\n    DATA_DIR = BASE_DIR \n    print(f\"Warning: Base directory {BASE_DIR} not found. Check paths.\")\n\nprint(f'✓ Data directory: {DATA_DIR}')\n\n# Hyperparameters\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSAVE_DIR = './checkpoints'\nRESUME_FROM = None  # Example: './checkpoints/last_checkpoint.pth'\n\nif not os.path.exists(SAVE_DIR):\n    os.makedirs(SAVE_DIR)\n\nprint(f'✓ Device: {DEVICE}')\nprint(f'✓ Batch size: {BATCH_SIZE}')\nprint(f'✓ Epochs: {NUM_EPOCHS}')\n\n# --- 2. LOAD DATASET ---\nprint('\\nLoading dataset...')\n\n# NOTE: Ensure 'WebFaceDataset' class is defined in a previous cell!\ntry:\n    full_dataset = WebFaceDataset(DATA_DIR, augment=True, img_size=224)\nexcept NameError:\n    print(\"❌ Error: 'WebFaceDataset' is not defined. Please run the cell defining the dataset class first.\")\n    raise\n\ntotal_size = len(full_dataset)\ntrain_size = int(0.9 * total_size)\nval_size = total_size - train_size\n\nindices = np.arange(total_size)\nnp.random.shuffle(indices)\n\ntrain_dataset = Subset(full_dataset, indices[:train_size])\nval_dataset = Subset(full_dataset, indices[train_size:])\n\nprint(f'✓ Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n\n# --- 3. DATALOADERS ---\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=2, \n    pin_memory=True, \n    drop_last=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=2, \n    pin_memory=True\n)\n\nprint(f'✓ Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n\n# --- 4. SANITY CHECK ---\nprint('\\nTesting data loading...')\ntry:\n    test_batch = next(iter(train_loader))\n    print(f'✓ Batch image shape: {test_batch[\"image\"].shape}')\n    print(f'✓ Data loading successful!')\nexcept Exception as e:\n    print(f\"❌ Error loading batch: {e}\")\n    raise\n\n# --- 5. TRAINING LOOP ---\nprint('\\n' + '='*80)\nprint('🚀 STARTING TRAINING')\nprint('='*80 + '\\n')\n\n# NOTE: Ensure 'model' and 'train_model' are defined in previous cells!\ntry:\n    trained_model = train_model(\n        model, \n        train_loader, \n        val_loader, \n        num_epochs=NUM_EPOCHS, \n        device=DEVICE,\n        resume_from=RESUME_FROM,\n        save_dir=SAVE_DIR\n    )\n    \n    print('\\n' + '='*80)\n    print('✅ TRAINING COMPLETE!')\n    print('='*80)\n\nexcept NameError as e:\n    print(f\"❌ Error: Missing definitions. {e}\")\n    print(\"Ensure 'model' and 'train_model' function are defined and run before this block.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:15:37.165828Z","iopub.execute_input":"2026-01-20T23:15:37.166438Z","iopub.status.idle":"2026-01-20T23:15:54.277014Z","shell.execute_reply.started":"2026-01-20T23:15:37.166403Z","shell.execute_reply":"2026-01-20T23:15:54.275605Z"}},"outputs":[{"name":"stdout","text":"✓ Data directory: /kaggle/input/webface-112x112/webface_112x112\n✓ Device: cuda\n✓ Batch size: 32\n✓ Epochs: 50\n\nLoading dataset...\nLoading dataset from: /kaggle/input/webface-112x112/webface_112x112\n✓ Found 490623 images\n✓ Train: 441560, Val: 49063\n✓ Train batches: 13798, Val batches: 1534\n\nTesting data loading...\n✓ Batch image shape: torch.Size([32, 3, 224, 224])\n✓ Data loading successful!\n\n================================================================================\n🚀 STARTING TRAINING\n================================================================================\n\n\n============================================================\nStarting training from epoch 0\n============================================================\n\n\nEpoch 1/50\n------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2072715256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# NOTE: Ensure 'model' and 'train_model' are defined in previous cells!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     trained_model = train_model(\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3586114293.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device, resume_from, save_dir)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         train_loss = train_epoch(model, train_loader, optimizer, criterion, \n\u001b[0m\u001b[1;32m    531\u001b[0m                                 device, scaler, epoch+1)\n\u001b[1;32m    532\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3586114293.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, device, scaler, epoch)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3586114293.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             cls_loss = self.focal_loss(\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3586114293.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mbce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbce_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mfocal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3593\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3594\u001b[0m             \u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3595\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1204224, 2])) must be the same as input size (torch.Size([301056, 2]))"],"ename":"ValueError","evalue":"Target size (torch.Size([1204224, 2])) must be the same as input size (torch.Size([301056, 2]))","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# Auto-detect correct directory\nBASE_DIR = '/kaggle/input/webface-112x112'\nsubdirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\nDATA_DIR = os.path.join(BASE_DIR, subdirs[0])\n\nprint(f'✓ Using data directory: {DATA_DIR}')\n\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(f'✓ Device: {DEVICE}')\n\n# Create dataset\nprint('\\nLoading dataset...')\nfull_dataset = WebFaceDataset(DATA_DIR, augment=True, img_size=224)\n\nprint(f'✓ Total images: {len(full_dataset)}')\n\n# Train/Val split\ntotal_size = len(full_dataset)\ntrain_size = int(0.9 * total_size)\nval_size = total_size - train_size\n\nindices = np.arange(total_size)\nnp.random.shuffle(indices)\n\ntrain_dataset = Subset(full_dataset, indices[:train_size])\nval_dataset = Subset(full_dataset, indices[train_size:])\n\nprint(f'✓ Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                         num_workers=2, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                       num_workers=2, pin_memory=True)\n\nprint(f'✓ Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n\n# Test batch\ntest_batch = next(iter(train_loader))\nprint(f'✓ Batch shape: {test_batch[\"image\"].shape}')\n\n# Train\nprint('\\n🚀 Starting training...')\ntrained_model = train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T23:06:05.233925Z","iopub.execute_input":"2026-01-20T23:06:05.234547Z","iopub.status.idle":"2026-01-20T23:06:19.664568Z","shell.execute_reply.started":"2026-01-20T23:06:05.234511Z","shell.execute_reply":"2026-01-20T23:06:19.663600Z"}},"outputs":[{"name":"stdout","text":"✓ Using data directory: /kaggle/input/webface-112x112/webface_112x112\n✓ Device: cuda\n\nLoading dataset...\nLoading dataset from: /kaggle/input/webface-112x112/webface_112x112\n✓ Found 490623 images\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3106216257.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebFaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'✓ Total images: {len(full_dataset)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Train/Val split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'WebFaceDataset' has no len()"],"ename":"TypeError","evalue":"object of type 'WebFaceDataset' has no len()","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}